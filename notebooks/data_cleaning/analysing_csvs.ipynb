{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORlszuNEI9M2GMsPbnV91o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niranjana-08/AI-Ascent/blob/main/notebooks/data_cleaning/analysing_csvs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Objective:\n",
        "\n",
        "This notebook represents the final and most advanced stage of data preparation. The core goals are:\n",
        "\n",
        "\n",
        "*   Create a Master DataFrame: Merge the cleaned job data with the classified job categories.\n",
        "*   Engineer an 'AI Relevance' Score: Use a sophisticated Sentence Transformer model to calculate a score for each job, indicating how closely it relates to the concept of AI.\n",
        "*   Classify AI Role Tiers: Analyze the relevance scores to define thresholds and categorize each job as a 'Traditional Role', 'AI-Impacted Role', or 'Core AI Role'.\n",
        "*   Final Cleaning: Perform the last set of data cleaning tasks to create two polished, analysis-ready datasets.\n",
        "\n",
        "### Note:\n",
        "\n",
        "\n",
        "*   RUN THE CODE IN GPU MODE\n",
        "*   this code is to be run after job classification into main and sub categories\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IbrbsWS3X8o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Master DataFrame Creation"
      ],
      "metadata": {
        "id": "9HdaQyhsZVYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Installing Libraries and Mounting Drive"
      ],
      "metadata": {
        "id": "FYZAtGw0ZawD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers -q"
      ],
      "metadata": {
        "id": "hi5quzvChJpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHBS-3KN0E-a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import sys\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "Lii_uWwd1L5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Loading Processed Datasets"
      ],
      "metadata": {
        "id": "WWVNd_bdZiTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/My Drive/job-analysis/job-analysis-dataset/'\n",
        "cleaned_data_path = base_path + 'data_cleaning/cleaned_for_classification.csv'\n",
        "classified_data_path = base_path + 'classified_jobs/classified_jobs.csv'"
      ],
      "metadata": {
        "id": "WrG84UG91U2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = pd.read_csv(cleaned_data_path)\n",
        "classified_df = pd.read_csv(classified_data_path)\n",
        "print(\"Files loaded successfully.\")"
      ],
      "metadata": {
        "id": "CPaK9P8_1n9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Merging to Create the Master Analysis DataFrame"
      ],
      "metadata": {
        "id": "c6805L_NZool"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMerging new classification columns into the main DataFrame\")\n",
        "columns_to_add = ['job_id', 'main_category', 'sub_category', 'confidence_score']\n",
        "\n",
        "analysis_df = pd.merge(\n",
        "    cleaned_df,\n",
        "    classified_df[columns_to_add],\n",
        "    on='job_id',\n",
        "    how='left'\n",
        "    # Using left merge to keep all rows from the original cleaned_df\n",
        ")\n",
        "print(\"Merge complete.\")"
      ],
      "metadata": {
        "id": "vt7pdU-y11C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Master Analysis DataFrame is Ready! ---\")\n",
        "print(\"Displaying info to confirm all columns are now present:\")\n",
        "analysis_df.info()\n",
        "\n",
        "print(\"\\nDisplaying the first 5 rows of the final analysis table:\")\n",
        "analysis_df.head(50)"
      ],
      "metadata": {
        "id": "u559Ey7Z2NRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "pd.set_option('display.width', 1000)\n",
        "print(\"\\nDisplaying the first 50 rows of the final analysis table:\")\n",
        "display(analysis_df.head(5))"
      ],
      "metadata": {
        "id": "_CxNASZj8e-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Feature Engineering - AI Relevance Scoring"
      ],
      "metadata": {
        "id": "SxMyJqlGZxTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Setting Up the Sentence Transformer Model"
      ],
      "metadata": {
        "id": "yKLvJzPqZy0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import our AI keywords and load the all-MiniLM-L6-v2 Sentence Transformer model. This model is excellent at converting text into numerical vectors (embeddings) that capture its semantic meaning. We'll ensure it runs on the GPU for speed."
      ],
      "metadata": {
        "id": "OFH0V3ReZ1Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords_folder_path = '/content/drive/My Drive/job-analysis/job-analysis-dataset/keywords/'\n",
        "sys.path.append(keywords_folder_path)\n",
        "try:\n",
        "    from keywords_ai import AI_KEYWORDS\n",
        "    print(\"keywords_ai.py imported successfully.\")\n",
        "except (ImportError, FileNotFoundError) as e:\n",
        "    print(f\"Error: Could not import AI_KEYWORDS. Make sure 'keywords_ai.py' is in the correct folder.\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "8gzOV1L0jVtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(\"Sentence Transformer model loaded.\")"
      ],
      "metadata": {
        "id": "xlwGcfNsjZVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Creating and Encoding the Target 'AI Concept'"
      ],
      "metadata": {
        "id": "5ygndulvaFTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We combine all our AI-related keywords into a single paragraph. The model then encodes this paragraph into one target vector that numerically represents the \"concept of AI.\""
      ],
      "metadata": {
        "id": "l9diJImAaIaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_concept_paragraph = ' '.join(AI_KEYWORDS)\n",
        "print(\"\\nEncoding the 'AI Concept' paragraph into a vector...\")\n",
        "ai_concept_embedding = model.encode(ai_concept_paragraph, convert_to_tensor=True, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "ca4qVm0Kjg2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Encoding All Job Descriptions (Embedding)"
      ],
      "metadata": {
        "id": "TkcuHQfiaNoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the model processes every job's combined_text and converts each one into its own vector. This is the most time-consuming step."
      ],
      "metadata": {
        "id": "lYW2nNHnaPO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEncoding all job descriptions into vectors (this will take a while)...\")\n",
        "job_texts = analysis_df['combined_text'].astype(str).tolist()\n",
        "job_embeddings = model.encode(job_texts, convert_to_tensor=True, show_progress_bar=True, batch_size=32)\n",
        "print(\"Job encoding complete.\")"
      ],
      "metadata": {
        "id": "1r9nOObsjixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Calculating and Analyzing Scores"
      ],
      "metadata": {
        "id": "HptTRmuYaTlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use cosine similarity to compare each job vector to our target \"AI Concept\" vector. The result is a score from -1 to 1 (practically 0 to 1) where a higher score means the job text is semantically closer to the concept of AI."
      ],
      "metadata": {
        "id": "-xXtfm5_aUYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating AI relevance scores for all jobs\")\n",
        "cosine_scores = util.pytorch_cos_sim(job_embeddings, ai_concept_embedding)\n",
        "\n",
        "analysis_df['ai_relevance_score'] = cosine_scores.cpu().numpy().flatten()\n",
        "print(\"Scores calculated and added to the DataFrame.\")\n",
        "top_ai_jobs = analysis_df.sort_values(by='ai_relevance_score', ascending=False)"
      ],
      "metadata": {
        "id": "KehSWlpgjlCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Displaying the top 15 most AI-relevant jobs found:\")\n",
        "display(top_ai_jobs[['title', 'main_category', 'ai_relevance_score']].head(15))"
      ],
      "metadata": {
        "id": "fbp5GoYanNKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"Displaying the first 5 rows with all columns:\")\n",
        "display(analysis_df.head(5))"
      ],
      "metadata": {
        "id": "V3p4yhWinXn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Exploratory Analysis - Determining AI Role Thresholds"
      ],
      "metadata": {
        "id": "uQrG9tWfabBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can classify jobs into tiers, we need to understand how the ai_relevance_score is distributed across different job categories. This exploratory analysis helps us set informed, data-driven thresholds."
      ],
      "metadata": {
        "id": "6h2UP2BEaeP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Analyzing Score Distributions by Category"
      ],
      "metadata": {
        "id": "Eg2yokX2agR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll loop through each main job category, plotting a histogram of its AI relevance scores and printing descriptive statistics. This gives us a clear picture of what a \"high\" or \"low\" score looks like for each field."
      ],
      "metadata": {
        "id": "wZepuOaGanup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories_to_analyze = analysis_df[analysis_df['main_category'] != 'Other']['main_category'].unique()\n",
        "\n",
        "print(f\"Starting individual analysis for {len(categories_to_analyze)} main categories...\")\n",
        "for category in categories_to_analyze:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"ANALYSIS FOR: {category}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    category_df = analysis_df[analysis_df['main_category'] == category]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(category_df['ai_relevance_score'], bins=30, kde=True)\n",
        "    plt.title(f'Distribution of AI Relevance Scores for {category}')\n",
        "    plt.xlabel('AI Relevance Score')\n",
        "    plt.ylabel('Number of Jobs')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n--- Descriptive Statistics for {category} ---\")\n",
        "    score_stats = category_df['ai_relevance_score'].describe(percentiles=[.25, .5, .75, .90, .95, .99])\n",
        "    print(score_stats)\n",
        "\n",
        "    print(f\"\\n--- Example Job Titles for {category} ---\")\n",
        "\n",
        "    def show_examples_for_category(df, score_min, score_max, num_examples=3):\n",
        "        \"\"\"Filters a category-specific DataFrame for a score range and shows examples.\"\"\"\n",
        "        sample = df[(df['ai_relevance_score'] >= score_min) & (df['ai_relevance_score'] < score_max)]\n",
        "        print(f\"\\nExamples with scores between {score_min} and {score_max}:\")\n",
        "        if sample.empty:\n",
        "            print(\"No jobs found in this score range.\")\n",
        "        else:\n",
        "            display(sample[['title', 'sub_category', 'ai_relevance_score']].head(num_examples))\n",
        "\n",
        "    show_examples_for_category(category_df, 0.2, 0.3)\n",
        "    show_examples_for_category(category_df, 0.4, 0.5)\n",
        "    show_examples_for_category(category_df, 0.6, 1.0)\n",
        "\n",
        "print(\"\\n\\n--- Individual analysis for all categories is complete. ---\")\n"
      ],
      "metadata": {
        "id": "c7bFLYjHn2zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Manual Inspection of Borderline Cases ( per job field )"
      ],
      "metadata": {
        "id": "x8MAcjZkaz61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hello"
      ],
      "metadata": {
        "id": "zss0VlZCfn5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_df = analysis_df[analysis_df['main_category'] == 'Media & Journalism']\n",
        "borderline_media_jobs = media_df[\n",
        "    (media_df['ai_relevance_score'] >= 0.28) &\n",
        "    (media_df['ai_relevance_score'] < 0.32)\n",
        "]\n",
        "\n",
        "print(\"--- Borderline Jobs (Scores between 0.28 and 0.32) for Media & Journalism ---\")\n",
        "display(borderline_media_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "iYEpiT8mkINP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "education_df = analysis_df[analysis_df['main_category'] == 'Education & EdTech']\n",
        "borderline_education_jobs = education_df[\n",
        "    (education_df['ai_relevance_score'] >= 0.35) &\n",
        "    (education_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "\n",
        "print(\"--- Borderline Jobs (Scores between 0.35 and 0.40) for Education & EdTech ---\")\n",
        "display(borderline_education_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "OoMJj6Esi6sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finance_df = analysis_df[analysis_df['main_category'] == 'Finance']\n",
        "\n",
        "borderline_finance_jobs = finance_df[\n",
        "    (finance_df['ai_relevance_score'] >= 0.35) &\n",
        "    (finance_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "\n",
        "print(\"--- Borderline Jobs (Scores between 0.35 and 0.40) for Finance ---\")\n",
        "display(borderline_finance_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "p7kjgABliESu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automotive_df = analysis_df[analysis_df['main_category'] == 'Automotive']\n",
        "borderline_automotive_jobs = automotive_df[\n",
        "    (automotive_df['ai_relevance_score'] >= 0.35) &\n",
        "    (automotive_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "print(\"--- Borderline Jobs (Scores between 0.35 and 0.40) for Automotive ---\")\n",
        "display(borderline_automotive_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "Y0jcnqcNhKRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "design_df = analysis_df[analysis_df['main_category'] == 'Design']\n",
        "borderline_design_jobs = design_df[\n",
        "    (design_df['ai_relevance_score'] >= 0.35) &\n",
        "    (design_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "\n",
        "print(f\"--- Found {len(borderline_design_jobs)} Borderline Jobs (Scores between 0.35 and 0.40) for Design ---\")\n",
        "display(borderline_design_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "RH378qcdfnI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "design_df = analysis_df[analysis_df['main_category'] == 'Design']\n",
        "borderline_design_jobs = design_df[\n",
        "    (design_df['ai_relevance_score'] >= 0.35) &\n",
        "    (design_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "\n",
        "print(\"--- Borderline Jobs (Scores between 0.35 and 0.40) for Design ---\")\n",
        "display(borderline_design_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "-j3bl1z3e1hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technology_df = analysis_df[analysis_df['main_category'] == 'Technology'].copy()\n",
        "score_bins = [-float('inf'), 0.28, 0.55, float('inf')]\n",
        "tier_labels = [\n",
        "    'Traditional Role',\n",
        "    'AI-Impacted Role',\n",
        "    'Core AI Role'\n",
        "]\n",
        "technology_df['ai_role_tier_check'] = pd.cut(\n",
        "    technology_df['ai_relevance_score'],\n",
        "    bins=score_bins,\n",
        "    labels=tier_labels,\n",
        "    right=False\n",
        ")\n",
        "tier_counts = technology_df['ai_role_tier_check'].value_counts()\n",
        "print(\"--- Job Distribution for Technology using new thresholds ---\")\n",
        "print(tier_counts)"
      ],
      "metadata": {
        "id": "yzxZHRAad42W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marketing_df = analysis_df[analysis_df['main_category'] == 'Marketing']\n",
        "borderline_marketing_jobs = marketing_df[\n",
        "    (marketing_df['ai_relevance_score'] >= 0.30) &\n",
        "    (marketing_df['ai_relevance_score'] < 0.35)\n",
        "]\n",
        "print(\"--- Borderline Jobs (Scores between 0.30 and 0.35) for Marketing ---\")\n",
        "display(borderline_marketing_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "4Hi9hrUpb-z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "legal_df = analysis_df[analysis_df['main_category'] == 'Legal']\n",
        "\n",
        "borderline_legal_jobs = legal_df[\n",
        "    (legal_df['ai_relevance_score'] >= 0.28) &\n",
        "    (legal_df['ai_relevance_score'] < 0.32)\n",
        "]\n",
        "print(\"--- Borderline Jobs (Scores around 0.30) for Legal ---\")\n",
        "display(borderline_legal_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "CKfE8CNka_5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "healthcare_df = analysis_df[analysis_df['main_category'] == 'Healthcare (Research & Admin)']\n",
        "borderline_healthcare_jobs = healthcare_df[\n",
        "    (healthcare_df['ai_relevance_score'] >= 0.35) &\n",
        "    (healthcare_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "print(\"--- Borderline Jobs (Scores between 0.35 and 0.40) for Healthcare ---\")\n",
        "display(borderline_healthcare_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "pssQfblPZf_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hr_df = analysis_df[analysis_df['main_category'] == 'Human Resources']\n",
        "borderline_hr_jobs = hr_df[\n",
        "    (hr_df['ai_relevance_score'] >= 0.30) &\n",
        "    (hr_df['ai_relevance_score'] < 0.35)\n",
        "]\n",
        "print(\"--- Borderline Jobs (Scores between 0.30 and 0.35) for Human Resources ---\")\n",
        "display(borderline_hr_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "H-56bYc_XrB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consulting_df = analysis_df[analysis_df['main_category'] == 'Consulting & Strategy']\n",
        "\n",
        "borderline_jobs = consulting_df[\n",
        "    (consulting_df['ai_relevance_score'] >= 0.35) &\n",
        "    (consulting_df['ai_relevance_score'] < 0.40)\n",
        "]\n",
        "print(\"--- Borderline Jobs (Scores between 0.35 and 0.40) for Consulting & Strategy ---\")\n",
        "display(borderline_jobs[['title', 'sub_category', 'ai_relevance_score']])"
      ],
      "metadata": {
        "id": "efDIQDrIWpB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Classifying Jobs into AI Tiers"
      ],
      "metadata": {
        "id": "rBibZUl1bZWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Defining Category-Specific Thresholds"
      ],
      "metadata": {
        "id": "i1tOyKRrbcVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a dictionary to hold the unique thresholds for each job category. A default is used for any category not specified. float('inf') is used where a category has no 'Core AI' roles."
      ],
      "metadata": {
        "id": "wPuYa5wSbetI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "new_category_thresholds = {}\n",
        "categories = analysis_df[analysis_df['main_category'] != 'Other']['main_category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    category_df = analysis_df[analysis_df['main_category'] == category]\n",
        "    impacted_threshold = round(category_df['ai_relevance_score'].quantile(0.75), 2)\n",
        "    core_ai_threshold = round(category_df['ai_relevance_score'].quantile(0.95), 2)\n",
        "    if core_ai_threshold <= impacted_threshold:\n",
        "        core_ai_threshold = impacted_threshold + 0.01\n",
        "    new_category_thresholds[category] = {\n",
        "        'traditional': impacted_threshold,\n",
        "        'impacted': core_ai_threshold\n",
        "    }\n",
        "\n",
        "new_category_thresholds['Technology'] = {'traditional': 0.28, 'impacted': 0.55}\n",
        "default_thresholds = {'traditional': 0.40, 'impacted': 0.60}"
      ],
      "metadata": {
        "id": "CFTlNvHiKVMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Applying the Classification Logic"
      ],
      "metadata": {
        "id": "AbSL-ypKbzCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function applies our custom thresholds to each row in the DataFrame to assign an ai_role_type. We also include a special rule to ensure no 'Technology' job is classified as purely 'Traditional'."
      ],
      "metadata": {
        "id": "sMOz0ZhWbz1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_ai_tier_new(row):\n",
        "    main_cat = row['main_category']\n",
        "    score = row['ai_relevance_score']\n",
        "    thresholds = new_category_thresholds.get(main_cat, default_thresholds)\n",
        "    if score >= thresholds['impacted']:\n",
        "        return 'Core AI Role'\n",
        "    elif score >= thresholds['traditional']:\n",
        "        return 'AI-Impacted Role'\n",
        "    else:\n",
        "        return 'Traditional Role'\n",
        "\n",
        "analysis_df['ai_role_type'] = analysis_df.apply(classify_ai_tier_new, axis=1)\n",
        "\n",
        "tech_upgrade_mask = (analysis_df['main_category'] == 'Technology') & (analysis_df['ai_role_type'] == 'Traditional Role')\n",
        "if tech_upgrade_mask.sum() > 0:\n",
        "    analysis_df.loc[tech_upgrade_mask, 'ai_role_type'] = 'AI-Impacted Role'\n",
        "    print(f\"Applied special rule: Upgraded {tech_upgrade_mask.sum()} 'Technology' jobs from 'Traditional' to 'AI-Impacted'.\")\n",
        "\n",
        "print(\"\\n--- Final Classification is Complete! ---\")\n",
        "print(\"\\nFinal distribution of jobs across the three tiers:\")\n",
        "print(analysis_df['ai_role_type'].value_counts())\n",
        "\n",
        "print(\"\\nSample of the final DataFrame with the correct 'ai_role_type' column:\")\n",
        "display(analysis_df[['title', 'main_category', 'ai_relevance_score', 'ai_role_type']].head(15))"
      ],
      "metadata": {
        "id": "X7U5K5vVLm1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Preparing Final DataFrames for Downstream Use"
      ],
      "metadata": {
        "id": "F0ruUlZOb8CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our main analysis and feature engineering are done, we will select the final columns needed and create two separate, clean DataFrames: one for dashboarding (dashboard_df) and one for future modeling (modeling_df)."
      ],
      "metadata": {
        "id": "tWY1ektNb-rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning : checking how ready the columns are"
      ],
      "metadata": {
        "id": "lIxBYDH3qUP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_list = analysis_df.columns.tolist()\n",
        "print(\"--- List of All Columns ---\")\n",
        "print(column_list)\n",
        "\n",
        "print(\"\\n\\n--- Detailed Summary of Each Column ---\")\n",
        "analysis_df.info()"
      ],
      "metadata": {
        "id": "LUomY8bbqXmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working on selecting the req columns which will be used ahead in analysis\n",
        "will be then performing complete data cleaning on all the these selected columns taken in a new df\n",
        "\n",
        "*Note : If any column is missing and edit this code at later point of time to include req column when neeed and run the complete code*"
      ],
      "metadata": {
        "id": "SUMT85FLytiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "if 'original_listed_time' in analysis_df.columns:\n",
        "    analysis_df['date_posted'] = pd.to_datetime(analysis_df['original_listed_time'] / 1000, unit='s').dt.date\n",
        "else:\n",
        "    analysis_df['date_posted'] = pd.NaT\n",
        "\n",
        "final_columns_for_dashboard = [\n",
        "    'job_id', 'title', 'company_name', 'location', 'date_posted',\n",
        "    'main_category', 'sub_category', 'ai_role_type', 'ai_relevance_score',\n",
        "    'formatted_experience_level', 'min_salary', 'med_salary', 'max_salary',\n",
        "    'pay_period', 'currency', 'cleaned_skills'\n",
        "]\n",
        "dashboard_df = analysis_df[final_columns_for_dashboard].copy()\n",
        "print(\"--- Final DataFrame for Dashboard is Ready! ---\")\n",
        "dashboard_df.info()\n",
        "\n",
        "print(\"\\nSample of the final dashboard data:\")\n",
        "display(dashboard_df.head())"
      ],
      "metadata": {
        "id": "rluNSCLKzC0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'original_listed_time' in analysis_df.columns:\n",
        "    analysis_df['date_posted'] = pd.to_datetime(analysis_df['original_listed_time'] / 1000, unit='s').dt.date\n",
        "else:\n",
        "    analysis_df['date_posted'] = pd.NaT\n",
        "\n",
        "final_columns = [\n",
        "    'job_id', 'title', 'company_name', 'location', 'date_posted',\n",
        "    'main_category', 'sub_category', 'ai_role_type', 'ai_relevance_score',\n",
        "    'formatted_experience_level', 'min_salary', 'med_salary', 'max_salary',\n",
        "    'pay_period', 'currency', 'cleaned_skills', 'combined_text'\n",
        "]\n",
        "dashboard_df = analysis_df[final_columns].copy()\n",
        "print(\"--- Final DataFrame for Dashboard & Modeling is Ready! ---\")\n",
        "dashboard_df.info()\n",
        "\n",
        "print(\"\\nSample of the final data:\")\n",
        "display(dashboard_df.head())"
      ],
      "metadata": {
        "id": "qmcODQMb3ayy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "above includes combined text"
      ],
      "metadata": {
        "id": "fm9Z6ffC36Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'date_posted' not in analysis_df.columns and 'original_listed_time' in analysis_df.columns:\n",
        "    analysis_df['date_posted'] = pd.to_datetime(analysis_df['original_listed_time'] / 1000, unit='s').dt.date\n",
        "\n",
        "dashboard_columns = [\n",
        "    'job_id', 'title', 'company_name', 'location', 'date_posted',\n",
        "    'main_category', 'sub_category', 'ai_role_type', 'ai_relevance_score',\n",
        "    'formatted_experience_level', 'min_salary', 'med_salary', 'max_salary',\n",
        "    'pay_period', 'currency', 'cleaned_skills'\n",
        "]\n",
        "dashboard_df = analysis_df[dashboard_columns].copy()\n",
        "modeling_columns = dashboard_columns + ['combined_text']\n",
        "modeling_df = analysis_df[modeling_columns].copy()\n",
        "\n",
        "print(\"--- DataFrames are Ready ---\")\n",
        "print(\"\\n1. dashboard_df (without combined_text):\")\n",
        "print(\"This is the lean DataFrame for analysis and visualization.\")\n",
        "display(dashboard_df.head(2))\n",
        "\n",
        "print(\"\\n2. modeling_df (with combined_text):\")\n",
        "print(\"This is the larger DataFrame for future model building.\")\n",
        "display(modeling_df.head(2))"
      ],
      "metadata": {
        "id": "koh0jdqB5Af1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for reference"
      ],
      "metadata": {
        "id": "E5lvHHtE53pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame for future modeling (contains 'combined_text'): modeling_df\")\n",
        "print(\"DataFrame for dashboarding and analysis (does not contain 'combined_text'): dashboard_df\")"
      ],
      "metadata": {
        "id": "qj0GnHR055Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Final Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "gHYuZKp2cWtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working initially with dashboard_df"
      ],
      "metadata": {
        "id": "-lDgyLcZslv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "focusing on column one by one"
      ],
      "metadata": {
        "id": "RozIL6Gq6fmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(dashboard_df.head(5))"
      ],
      "metadata": {
        "id": "98PJODU96wTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "currency column"
      ],
      "metadata": {
        "id": "rC7AEXcUdGBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "currency_counts = dashboard_df['currency'].value_counts()\n",
        "\n",
        "print(\"Currency distribution in the dataset:\")\n",
        "print(currency_counts)"
      ],
      "metadata": {
        "id": "6SkERPwv56Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversion_rates = {\n",
        "    'EUR': 1.08,  # Euro to USD\n",
        "    'CAD': 0.73,  # Canadian Dollar to USD\n",
        "    'BBD': 0.50,  # Barbadian Dollar to USD\n",
        "    'AUD': 0.66,  # Australian Dollar to USD\n",
        "    'GBP': 1.27,  # British Pound to USD\n",
        "    'USD': 1.00   # USD to USD is 1\n",
        "}\n",
        "print(\"Conversion rates defined.\")\n",
        "\n",
        "dashboard_df['conversion_rate'] = dashboard_df['currency'].map(conversion_rates)\n",
        "dashboard_df['conversion_rate'].fillna(1.0, inplace=True)\n",
        "salary_cols = ['min_salary', 'med_salary', 'max_salary']\n",
        "for col in salary_cols:\n",
        "    dashboard_df[col] = dashboard_df[col] * dashboard_df['conversion_rate']\n",
        "\n",
        "print(\"Salaries converted to USD.\")\n",
        "dashboard_df.drop(columns=['currency', 'conversion_rate'], inplace=True)\n",
        "print(\"Cleaned up temporary columns.\")\n",
        "\n",
        "print(\"\\n--- Salary Conversion Complete! ---\")\n",
        "display(dashboard_df[dashboard_df['job_id'].isin([283, 294])])\n",
        "display(dashboard_df.head())"
      ],
      "metadata": {
        "id": "1ozd8cZ47sF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame Info:\")\n",
        "dashboard_df.info()\n",
        "\n",
        "print(\"\\nUpdated Salary Statistics (now all in USD):\")\n",
        "print(dashboard_df[['min_salary', 'med_salary', 'max_salary']].describe())"
      ],
      "metadata": {
        "id": "-L0XNDNM_d2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "date_posted column removed"
      ],
      "metadata": {
        "id": "crT0yAF9_7cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 'date_posted' column from the DataFrame\n",
        "# dashboard_df.drop(columns=['date_posted'], inplace=True)"
      ],
      "metadata": {
        "id": "dSahxLzs_9Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_df.head()"
      ],
      "metadata": {
        "id": "nxYEAZLvAHjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "formatted_experience_level -> filling null with not specified"
      ],
      "metadata": {
        "id": "G9TKvZR1BBDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_df['formatted_experience_level'] = dashboard_df['formatted_experience_level'].fillna('Not Specified')\n",
        "\n",
        "print(\"Missing values have been filled. Here are the new counts for each experience level:\")\n",
        "print(dashboard_df['formatted_experience_level'].value_counts())"
      ],
      "metadata": {
        "id": "rwX7T8NnAOPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_df['formatted_experience_level'].info()"
      ],
      "metadata": {
        "id": "VnrpMkvnCLKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "updating unknown salary columns with zero, while using it in analysis we will consider only rows greater than 0\n"
      ],
      "metadata": {
        "id": "Juux3knKJu2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaning salary-related columns...\")\n",
        "salary_cols = ['min_salary', 'med_salary', 'max_salary']\n",
        "\n",
        "for col in salary_cols:\n",
        "    dashboard_df[col] = dashboard_df[col].fillna(0)\n",
        "dashboard_df['pay_period'] = dashboard_df['pay_period'].fillna('Not Specified')\n",
        "\n",
        "print(\"Salary columns cleaned.\")\n",
        "print(\"\\n--- Verifying the cleaned DataFrame ---\")\n",
        "dashboard_df.info()"
      ],
      "metadata": {
        "id": "2fMUkrNPCPZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_df['company_name'] = dashboard_df['company_name'].fillna('Unknown')\n",
        "print(\"Missing company names have been filled.\")\n",
        "print(\"\\nVerifying the DataFrame (note the 'company_name' non-null count):\")\n",
        "dashboard_df.info()"
      ],
      "metadata": {
        "id": "VwLAmrJ2vgwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filling unknown cleaned_skills section with blank string"
      ],
      "metadata": {
        "id": "y46WSjnTwTGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_df['cleaned_skills'] = dashboard_df['cleaned_skills'].fillna('')\n",
        "print(\"Missing skills have been filled.\")\n",
        "print(\"\\nVerifying the DataFrame (note the 'cleaned_skills' non-null count):\")\n",
        "dashboard_df.info()"
      ],
      "metadata": {
        "id": "7juOaOrrwhfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_df = dashboard_df.drop(columns=['date_posted'])\n",
        "\n",
        "print(\"The 'date_posted' column has been removed.\")\n",
        "dashboard_df.info()"
      ],
      "metadata": {
        "id": "c1bSVuo1w6XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### adding combined_text as an extra column in modelling_df derived from dashboard_df"
      ],
      "metadata": {
        "id": "FMXoIzB6ti0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeling_df = dashboard_df.copy()\n",
        "modeling_df['combined_text'] = analysis_df['combined_text']\n",
        "\n",
        "print(\"--- Updated 'modeling_df' is Ready! ---\")\n",
        "print(\"It now contains the clean dashboard columns plus 'combined_text'.\")\n",
        "modeling_df.info()\n",
        "print(\"\\nSample of the new modeling_df:\")\n",
        "display(modeling_df.head())"
      ],
      "metadata": {
        "id": "qkqQoFN6xS7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Saving the Analysis-Ready Datasets"
      ],
      "metadata": {
        "id": "cpUDHfiLdTep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder_path = '/content/drive/My Drive/job-analysis/job-analysis-dataset/data_cleaning/'\n",
        "dashboard_file_path = output_folder_path + 'analysis_ready_without_combinedtext.csv'\n",
        "modeling_file_path = output_folder_path + 'analysis_ready_with_combinedtext.csv'\n",
        "\n",
        "print(f\"Saving the dashboard DataFrame to: {dashboard_file_path}\")\n",
        "dashboard_df.to_csv(dashboard_file_path, index=False)\n",
        "print(\"dashboard_df saved successfully.\")\n",
        "\n",
        "print(f\"\\nSaving the modeling DataFrame to: {modeling_file_path}\")\n",
        "modeling_df.to_csv(modeling_file_path, index=False)\n",
        "print(\"modeling_df saved successfully.\")"
      ],
      "metadata": {
        "id": "JJ_rQE_mzGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_rows, dashboard_cols = dashboard_df.shape\n",
        "print(f\"dashboard_df has:\")\n",
        "print(f\"- {dashboard_rows} rows\")\n",
        "print(f\"- {dashboard_cols} columns\")\n",
        "\n",
        "modeling_rows, modeling_cols = modeling_df.shape\n",
        "print(f\"\\nmodeling_df has:\")\n",
        "print(f\"- {modeling_rows} rows\")\n",
        "print(f\"- {modeling_cols} columns\")"
      ],
      "metadata": {
        "id": "-Ba9VhlZ0aH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Post-Processing - Adding State Codes"
      ],
      "metadata": {
        "id": "A68IKa6NdXv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an enhancement, this section re-loads the saved files to add a state_code column(two-letter state code), which is useful for geographical analysis. This is done by extracting two-letter codes from the location string."
      ],
      "metadata": {
        "id": "BSIDAO2MdkU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "base_path = '/content/drive/My Drive/job-analysis/job-analysis-dataset/'\n",
        "data_folder = base_path + 'data_cleaning/'\n",
        "dashboard_file_path = data_folder + 'analysis_ready_without_combinedtext.csv'\n",
        "modeling_file_path = data_folder + 'analysis_ready_with_combinedtext.csv'\n",
        "\n",
        "try:\n",
        "    dashboard_df = pd.read_csv(dashboard_file_path)\n",
        "    modeling_df = pd.read_csv(modeling_file_path)\n",
        "    print(\"Both DataFrames loaded successfully!\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\\\nError: A file was not found. Please check your file paths.\")\n",
        "    raise e\n",
        "\n",
        "def get_state_code(location):\n",
        "    \"\"\"A robust function to extract a two-letter state code.\"\"\"\n",
        "    if not isinstance(location, str):\n",
        "        return 'None'\n",
        "\n",
        "    parts = location.split(',')\n",
        "\n",
        "    if len(parts) > 1:\n",
        "        potential_state = parts[-1].strip()\n",
        "\n",
        "        if len(potential_state) == 2 and potential_state.isalpha() and potential_state.isupper():\n",
        "            return potential_state\n",
        "    return 'None'\n",
        "\n",
        "print(\"\\\\nAdding 'state_code' column to the dashboard DataFrame...\")\n",
        "dashboard_df['state_code'] = dashboard_df['location'].apply(get_state_code)\n",
        "\n",
        "print(\"Adding 'state_code' column to the modeling DataFrame...\")\n",
        "modeling_df['state_code'] = modeling_df['location'].apply(get_state_code)\n",
        "\n",
        "try:\n",
        "    dashboard_df.to_csv(dashboard_file_path, index=False)\n",
        "    modeling_df.to_csv(modeling_file_path, index=False)\n",
        "    print(\"\\\\nSuccess! Both CSV files have been updated with the 'state_code' column and saved.\")\n",
        "    print(f\"-> {dashboard_file_path}\")\n",
        "    print(f\"-> {modeling_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\\\nAn error occurred while saving the files: {e}\")\n",
        "\n",
        "state_code_counts = dashboard_df['state_code'].value_counts()\n",
        "none_count = state_code_counts.get('None', 0)\n",
        "state_codes_only_counts = state_code_counts.drop('None', errors='ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Detailed State Code Verification:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n--- Occurrences of Each State Code ---\\n\")\n",
        "print(state_codes_only_counts)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"\\nTotal number of jobs with no state code found (written as 'None'): {none_count}\")\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "8KG9u0R6KLcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}